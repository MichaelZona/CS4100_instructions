{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VM4y2x7StzcB",
        "YiUjoZDjhIh7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## LLM inference\n",
        "\n",
        "We plan to load the `Llama-1B` model from *HuggingFace* and run inference by entering a prompt and generating a response.\n",
        "\n",
        "We start the tutorial with python environment and packages. The Python environment refers to the setup in which we run our code, including the interpreter, the libraries, and the tools that make development easier."
      ],
      "metadata": {
        "id": "k4CLxKC8pmcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment and packages\n",
        "The Python environment is like your toolbox. By itself, it only has a few basic tools. When you want to do more, such as calculating the sum, mean, or variance, you need to add new tools. These extra tools come in packages. A package is like a small toolbox dedicated to a certain task. By downloading and installing the corresponding packages, you equip your Python environment with everything needed to complete your project."
      ],
      "metadata": {
        "id": "b1GkYN_7_JKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The basic package used in LLMs is `transformers`. Your can run following command to install it."
      ],
      "metadata": {
        "id": "4Ovmpygh_CP1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8VbekmXpKR8"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*HuggingFace* is an opensourced platform like GitHub to offer models, datasets, and papers. You need to sign up for an account and create a new token in the [link](https://huggingface.co/settings/tokens).\n",
        "\n",
        "To load the models from *HuggingFace*, please fill in the table to access the model in the [link](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct). It may take a few minutes to process and get the permission. Then, you need to log in `huggingface_cli` with your own token.\n"
      ],
      "metadata": {
        "id": "VM4y2x7StzcB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4c89f4b",
        "collapsed": true
      },
      "source": [
        "!huggingface-cli login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now, you can load the model from *Huggingface*\n",
        "\n",
        "We load the tokenizer and model of `Llama-3.2-1B-Instruct`."
      ],
      "metadata": {
        "id": "qCqP4pAudLVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "G9UdWuYoyX5A",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The inference procedure can be divided into three parts.\n",
        "\n",
        "First, the natural language input needs to be converted into a form that the model understands. This is the job of the tokenizer: it splits sentences into tokens and maps them to the corresponding indices. For example, the sentence `\"I want to learn more about AI\"` might be converted into a list of numbers like `[42, 103, 88, 44]`.\n",
        "\n",
        "Second, we feed these token indices into the model and obtain the output using the `model.generate()` function. The model may produce another list of numbers, such as `[42, 103, 88, 44, 205, 77]`, which represents the predicted tokens.\n",
        "\n",
        "Finally, we decode this output back into human-readable text using `tokenizer.decode()`. In this example, `[42, 103, 88, 42, 205, 77]` would be decoded into `\"I want to learn more about AI and NLP.\"`.\n"
      ],
      "metadata": {
        "id": "zilrXkYCd1mE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bbb3f9c",
        "collapsed": true
      },
      "source": [
        "input_text = \"Hello, I'm your TA. Welcome to CS4100 and welcome back to the campus!\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated answer:\", output_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also use `model().logits` to get the original model output logit. This logit is before the softmax layer. The `inputs` here contains two keys: `\"input_ids\"` and `\"attention_mask\"`. `\"attention_mask\"` is related to the padding. At this part, we don't have any padding procedure, so we just focus on `\"inputs_id\"`. It's an ordered list of numbers, indicating the whole input sentences. The logits are used to get the inferred output in the above response."
      ],
      "metadata": {
        "id": "YiUjoZDjhIh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Hello, I'm your TA. Welcome to CS4100 and welcome back to the campus!\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model(inputs[\"input_ids\"])\n",
        "logits = outputs.logits\n",
        "print(logits.size())"
      ],
      "metadata": {
        "id": "08jxOPVipVfP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}